{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The `ImageDataGenerator` in Keras converts real images into matrices of pixels. When working with image data in deep learning, it is common to represent images as numerical data in the form of matrices or tensors.\n",
        "\n",
        "- The `ImageDataGenerator` class in Keras provides methods for loading and preprocessing image data from directories. When you use the `ImageDataGenerator` to load images, it automatically reads the image files, converts them into numerical matrices, and performs preprocessing steps as specified.\n",
        "\n",
        "Here's a brief overview of how the `ImageDataGenerator` works with image data:\n",
        "\n",
        "1. Loading Images: The `ImageDataGenerator` class provides methods such as `flow_from_directory` that reads images from a specified directory. It automatically traverses the directory structure, reads the image files, and loads them into memory.\n",
        "\n",
        "2. Image Preprocessing: The `ImageDataGenerator` can perform various preprocessing operations on the loaded images. This includes rescaling the pixel values, applying normalization, handling image resizing, and applying data augmentation techniques like rotation, shifting, and flipping.\n",
        "\n",
        "3. Batching and Conversion: The `ImageDataGenerator` typically loads images in batches, dividing the dataset into smaller subsets. Each image is then converted into a matrix or tensor representation, depending on the specific requirements of the deep learning model.\n",
        "\n",
        "The resulting matrices or tensors represent the images as numerical data suitable for input to a deep learning model. Each element of the matrix corresponds to a pixel value, and the dimensions of the matrix depend on the image size and the number of color channels (e.g., RGB or grayscale).\n",
        "\n",
        "By converting real pictures into matrices of pixels, deep learning models can process and learn from the pixel-level information. The matrices are then fed into the network, allowing the model to learn patterns, features, and relationships within the image data.\n",
        "\n",
        "In summary, the `ImageDataGenerator` in Keras reads real images, converts them into matrices or tensors of pixel values, and performs preprocessing operations on the image data. This prepares the image data for training deep learning models that can learn from and make predictions on image datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sCV30xyVhFbE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FIleuCAjoFD8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.13.0-rc2'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go through each parameter and understand its purpose:\n",
        "\n",
        "1. `rescale = 1./255`: This parameter rescales the pixel values of the images. Dividing each pixel value by 255 normalizes the values to be between 0 and 1. This rescaling is a common preprocessing step in deep learning to ensure that the input data falls within a desired range.\n",
        "\n",
        "2. `shear_range = 0.2`: This parameter specifies the range for shear transformations. Shearing involves shifting one part of the image along a certain direction, creating a tilt effect. The shear range of 0.2 means that the image can be sheared up to 20% of its width or height.\n",
        "\n",
        "3. `zoom_range = 0.2`: This parameter determines the range for random zooming. Zooming involves either enlarging or reducing the size of the image. The zoom range of 0.2 means that the image can be zoomed in or out by up to 20%.\n",
        "\n",
        "4. `horizontal_flip = True`: This parameter enables random horizontal flipping of the images. Horizontal flipping mirrors the image horizontally, creating a new image with the same features but in the opposite direction. This augmentation technique can help increase the diversity of the training data and make the model more robust to horizontal spatial variations.\n",
        "\n",
        "The `ImageDataGenerator` object, `train_datagen`, with these settings is typically used for data augmentation during the training phase of a deep learning model. Data augmentation introduces variations to the input images, artificially increasing the size and diversity of the training dataset. This can help improve the model's generalization ability and prevent overfitting.\n",
        "\n",
        "During training, the `train_datagen` object is often used in conjunction with the `flow_from_directory` method of the `ImageDataGenerator` class. This method generates augmented batches of images on-the-fly from a directory structure, allowing for efficient training with augmented data.\n",
        "\n",
        "The code you provided is used to generate a training set by loading and augmenting images from a directory. Let's break down the code and understand its functionality:\n",
        "\n",
        "1. `train_datagen.flow_from_directory('dataset/training_set'...`: This line specifies the directory path where the training images are located. The `flow_from_directory` method of the `ImageDataGenerator` class is used to load images from the specified directory and generate augmented batches of images.\n",
        "\n",
        "2. `target_size = (64, 64)`: This parameter specifies the desired size to which the input images should be resized. In this case, the images will be resized to a size of 64x64 pixels. Resizing the images to a consistent size is often necessary to ensure uniformity and compatibility with the deep learning model.\n",
        "\n",
        "3. `batch_size = 32`: This parameter defines the number of images in each batch. During training, the training set will be divided into batches, and each batch will contain 32 images. Batching allows for efficient processing and updating of model parameters during training.\n",
        "\n",
        "4. `class_mode = 'binary'`: This parameter specifies the type of problem or the format of the labels. In this case, the training data is expected to be categorized into two classes, hence the `'binary'` class mode. If you have multiple classes, you can use `'categorical'` as the class mode.\n",
        "\n",
        "\n",
        "In summary, the code snippet you provided initializes a training set by loading and augmenting images from a directory using an `ImageDataGenerator`. The generated training set is suitable for training a deep learning model, and the provided settings determine the augmentation techniques, target size, batch size, and class mode used during the process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0koUcJMJpEBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SH4WzfOhpKc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SAUt4UMPlhLS"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XPzPrMckl-hV"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ncpqPl69mOac"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i_-FZjn_m8gk"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6AZeOGCvnNZn"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8GtmUlLd26Nq"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1p_Zj1Mc3Ko_"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NALksrNQpUlJ"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XUj1W4PJptta"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.6696 - accuracy: 0.5841 - val_loss: 0.6017 - val_accuracy: 0.6815\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.5977 - accuracy: 0.6827 - val_loss: 0.5699 - val_accuracy: 0.7080\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.5608 - accuracy: 0.7126 - val_loss: 0.6291 - val_accuracy: 0.6750\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.5179 - accuracy: 0.7470 - val_loss: 0.6451 - val_accuracy: 0.6800\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.5010 - accuracy: 0.7556 - val_loss: 0.5444 - val_accuracy: 0.7370\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.4840 - accuracy: 0.7689 - val_loss: 0.4940 - val_accuracy: 0.7615\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.4792 - accuracy: 0.7699 - val_loss: 0.4913 - val_accuracy: 0.7670\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.4632 - accuracy: 0.7815 - val_loss: 0.4807 - val_accuracy: 0.7750\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.4459 - accuracy: 0.7943 - val_loss: 0.4934 - val_accuracy: 0.7670\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.4405 - accuracy: 0.7985 - val_loss: 0.4361 - val_accuracy: 0.8005\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.4126 - accuracy: 0.8091 - val_loss: 0.4490 - val_accuracy: 0.7975\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.4137 - accuracy: 0.8046 - val_loss: 0.4335 - val_accuracy: 0.8015\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.4080 - accuracy: 0.8144 - val_loss: 0.4226 - val_accuracy: 0.8015\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.3952 - accuracy: 0.8238 - val_loss: 0.4705 - val_accuracy: 0.7975\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.3853 - accuracy: 0.8250 - val_loss: 0.4349 - val_accuracy: 0.8030\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.3689 - accuracy: 0.8301 - val_loss: 0.4387 - val_accuracy: 0.8025\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.3610 - accuracy: 0.8394 - val_loss: 0.4443 - val_accuracy: 0.8100\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3540 - accuracy: 0.8411 - val_loss: 0.5135 - val_accuracy: 0.7730\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3513 - accuracy: 0.8447 - val_loss: 0.4337 - val_accuracy: 0.8075\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3320 - accuracy: 0.8531 - val_loss: 0.4654 - val_accuracy: 0.7940\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.3265 - accuracy: 0.8581 - val_loss: 0.4564 - val_accuracy: 0.8065\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3252 - accuracy: 0.8589 - val_loss: 0.4400 - val_accuracy: 0.8060\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.3093 - accuracy: 0.8633 - val_loss: 0.4360 - val_accuracy: 0.7990\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.2988 - accuracy: 0.8750 - val_loss: 0.5307 - val_accuracy: 0.7740\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.2961 - accuracy: 0.8709 - val_loss: 0.4448 - val_accuracy: 0.8120\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2b7fe6df0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gsSiWEJY1BPB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "[[0.99871]]\n",
            "dog\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Load and preprocess the test image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = test_image / 255.0  # Rescale the pixel values to [0, 1]\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Perform prediction on the test image using the trained CNN model\n",
        "result = cnn.predict(test_image)\n",
        "print(result)\n",
        "\"\"\"\n",
        "The line `test_image = np.expand_dims(test_image, axis=0)` is used to add an extra dimension to the test image array. \n",
        "\n",
        "In the context of the code snippet, `test_image` is initially a 3D array representing the image, where the dimensions are height, width, and channels. However, the `predict` method of the CNN model expects a batch of images as input, even if it's just a single image. Therefore, the array needs to have an additional dimension to indicate the batch size.\n",
        "\n",
        "By using `np.expand_dims(test_image, axis=0)`, a new dimension is added at `axis=0`, effectively creating a new axis for the batch size. This transforms the 3D array of shape (height, width, channels) into a 4D array of shape (1, height, width, channels), where the first dimension represents the batch size, which is 1 in this case.\n",
        "\n",
        "Adding this extra dimension allows the single test image to be passed as input to the CNN model's `predict` method, which expects a batch of images.\n",
        "\"\"\"\n",
        "\n",
        "# Get the class indices of the training set\n",
        "class_indices = training_set.class_indices\n",
        "threshold = 0.5\n",
        "# Determine the predicted label (dog or cat) based on the prediction result\n",
        "if result[0][0] > threshold:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "\n",
        "print(prediction)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next to do\n",
        "\n",
        "- How to save the trained model and allow the user to input the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ED9KB3I54c1i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "convolutional_neural_network.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
